{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to create a spatialite database from GMR inversions. Spatialite was chosen as the format as it is compact, easy to set up and readable by various software including GIS and python. Much of the creation depends on two spreadsheets referenced in this notebok which contain metadata, key spatial information and map inversion directories.\n",
    "\n",
    "Neil Symington\n",
    "neil.symington@gs.gov.au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.wkb\n",
    "import shapely.wkt\n",
    "import os\n",
    "import pandas as pd\n",
    "# sqlite/spatialite\n",
    "from sqlalchemy import create_engine, event, ForeignKey\n",
    "from sqlalchemy import Column, Integer, String, Float, Date, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlite3 import dbapi2 as sqlite\n",
    "from scipy.io import loadmat\n",
    "import sqlite3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d12729fbf395>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-d12729fbf395>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    elif user == 'Alex Ip'\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Configuration for local environment\n",
    "\n",
    "user = 'Neil Symington'\n",
    "#user = 'Alex Ip'\n",
    "\n",
    "if user == 'Neil Symington':\n",
    "    # Neil Symington's local configuration\n",
    "    DB_ROOT = r\"C:\\GA\\SNMR\"\n",
    "    SPATIALITE_PATH = r'C:\\Users\\symin\\mod_spatialite-NG-win-amd64\\mod_spatialite-NG-win-amd64'\n",
    "elif user == 'Alex Ip':\n",
    "    # Alex Ip's local configuration\n",
    "    DB_ROOT = r\"F:\\Groundwater\\SNMR\" # Alex's external hard drive\n",
    "    SPATIALITE_PATH = None # Not required - already in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spatialite extension for sqlite if required. \n",
    "# Make sure that mod_spatialite.dll is located in a folder that is in your system path\n",
    "# This will only work on windows computers\n",
    "\n",
    "if SPATIALITE_PATH:\n",
    "    os.environ['PATH'] = SPATIALITE_PATH + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = os.path.join(DB_ROOT, r\"East_Kimberley_SNMR.sqlite\")\n",
    "\n",
    "if os.path.exists(DB_PATH):\n",
    "        os.remove(DB_PATH)\n",
    "\n",
    "        \n",
    "engine = create_engine('sqlite:///' + DB_PATH, module=sqlite, echo=False)\n",
    "\n",
    "@event.listens_for(engine, 'connect')\n",
    "def connect(dbapi_connection, connection_rec):\n",
    "    dbapi_connection.enable_load_extension(True)\n",
    "    dbapi_connection.execute('SELECT load_extension(\"mod_spatialite\")')\n",
    "\n",
    "# create spatialite metadata\n",
    "print('creating spatial metadata...')\n",
    "engine.execute(\"SELECT InitSpatialMetaData(1);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class Sites(Base):\n",
    "    __tablename__ = 'sites'\n",
    "    site_id = Column(Integer, index=True, primary_key=True)\n",
    "    field_id = Column(\"Field_ID\", String(20))\n",
    "    site_code = Column(\"Site_Code\", String(40))\n",
    "    mid_x = Column(\"mid_X\", Float)\n",
    "    mid_y = Column(\"mid_Y\", Float)\n",
    "    declination = Column(\"declination_angle\", Float)\n",
    "    inclination = Column(\"inclination_angle\", Float)\n",
    "    date = Column(\"date\", Date)\n",
    "    loop_width = Column(\"loop_width\", Float)\n",
    "    coil_type = Column(\"coil_type\", String(20))\n",
    "    geometry = Column(String)\n",
    "      \n",
    "    \n",
    "class Acquisitions(Base):\n",
    "    __tablename__ = 'acquisitions'\n",
    "    acquisition_id = Column(Integer, index=True, primary_key=True)\n",
    "    pulse_sequence = Column(\"pulse_sequence\", String(5))\n",
    "    pulse_length = Column(\"pulse_length\", Float)\n",
    "    \n",
    "    \n",
    "    site_id = Column(Integer, ForeignKey('sites.site_id'))\n",
    "    sites = relationship(\"Sites\")\n",
    "\n",
    "    \n",
    "class Inverse_model_metadata(Base):\n",
    "    __tablename__ = 'inverse_model_metadata'\n",
    "    inversion_id = Column(Integer, index=True, primary_key=True)\n",
    "    doi = Column('Depth_of_Investigation', Float)\n",
    "    reg_factor = Column('reg_factor', Float)\n",
    "    cond_profile = Column(\"conductive_earth_model\", Boolean)\n",
    "    inversion_software = Column(\"Inversion_software\", String(50))\n",
    "    \n",
    "    \n",
    "    acquisition_id = Column(Integer, ForeignKey('acquisitions.acquisition_id'))\n",
    "    acquisitions = relationship(\"Acquisitions\")\n",
    "    \n",
    "class Inverse_models(Base):\n",
    "    __tablename__ = 'inverse_models'\n",
    "    table_id = Column(Integer, index=True, primary_key=True)\n",
    "    depth_from = Column('Depth_from', Float)\n",
    "    depth_to = Column('Depth_to', Float)\n",
    "    mobile_water_content = Column(\"Mobile_water_content\", Float)\n",
    "    bound_water_content = Column(\"Bound_water_content\", Float)\n",
    "    total_water_content = Column(\"Total_water_content\", Float)\n",
    "    T1 = Column(\"T1\", Float)\n",
    "    T2 = Column(\"T2\", Float)\n",
    "    T2_star = Column(\"T2*\", Float)\n",
    "    frequency = Column(\"frequency\", Float)\n",
    "    phase = Column(\"phase\", Float)\n",
    "    \n",
    "    site_id = Column(Integer, ForeignKey('sites.site_id'))\n",
    "    sites = relationship(\"Sites\")\n",
    "    \n",
    "    acquisition_id = Column(Integer, ForeignKey('acquisitions.acquisition_id'))\n",
    "    acquisitions = relationship(\"Acquisitions\")\n",
    "\n",
    "    \n",
    "    inversion_id = Column(Integer, ForeignKey('inverse_model_metadata.inversion_id'))\n",
    "    inversions = relationship(\"Inverse_model_metadata\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sites.__table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = os.path.join(DB_ROOT, r\"EK_SNMR_location_metadata.csv\")\n",
    "\n",
    "df = pd.read_csv(infile)\n",
    "\n",
    "df['date'] =pd.to_datetime(df.date, format =\"%d/%m/%Y\")\n",
    "\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed in the table\n",
    "\n",
    "\n",
    "df.rename(columns = {'X': 'mid_X', 'Y': 'mid_Y', 'bo_inc_angle_deg': 'inclination_angle',\n",
    "                     'bo_dec_angle_deg': 'declination_angle'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the data into a dataframe\n",
    "\n",
    "infile = os.path.join(DB_ROOT, r\"EK_conductive_earth_inversion_spreadsheet.csv\")\n",
    "df_acquisition = pd.read_csv(infile)\n",
    "\n",
    "df_acquisition.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add the location index and find it using a join on the site id column\n",
    "\n",
    "loc_id = -999 * np.ones(len(df_acquisition['Field_ID']),\n",
    "                      dtype = np.int64)\n",
    "\n",
    "for i , item in enumerate(df_acquisition['Field_ID'].values):\n",
    "    # find the location index\n",
    "    loc_id[i] = df[df['Field_ID'] == item].index[0]\n",
    "\n",
    "\n",
    "df_acquisition = df_acquisition.assign(site_id = loc_id)\n",
    "\n",
    "df_acquisition['pulse_length'].dtype\n",
    "\n",
    "df_acquisition = df_acquisition.sort_values(by='site_id').reset_index(drop=True)\n",
    "\n",
    "df_acquisition.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_acquisition['inversion_file'] = ''\n",
    "\n",
    "for index, row in df_acquisition.iterrows():\n",
    "    new_path = row['save_pathname'].replace(r'D:\\EastKimberley_SNMR\\SNMR_database', DB_ROOT)\n",
    "    \n",
    "    df_acquisition['inversion_file'].iloc[index] = new_path + row['save_filename'] + '\\\\' + row['save_filename'] + '_1d_inversion.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acquisition['inversion_file'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define headers for the various inversion files\n",
    "\n",
    "header_dict  = {}\n",
    "\n",
    "header_dict['FID'] = ['Depth_from', 'Depth_to', 'K_rel', 'water_content', 'T2*', 'frequency',\n",
    "          'phase', 'T1', 'Bound_water_content', 'Mobile_water_content', 'Total_water_content']\n",
    "\n",
    "header_dict['CPMG'] = ['Depth_from', 'Depth_to', 'K_rel', 'Mobile_water_content', 'T2']\n",
    "\n",
    "header_dict['T1'] = ['Depth_from', 'Depth_to', 'K_rel', 'water_content', 'T2*', 'frequency',\n",
    "          'phase', 'T1', 'Bound_water_content', 'Mobile_water_content', 'Total_water_content']\n",
    "\n",
    "# This function parses the text file automatically created by the inversion algorithm\n",
    "\n",
    "\n",
    "def parse_inversion_file(infile, pulse_sequence):\n",
    "    \n",
    "    '''\n",
    "    infile: the filename of the text file\n",
    "    sequence: ['FID', 'CPMG', 'T1']\n",
    "    '''\n",
    "    # Get header based on the sequence\n",
    "    \n",
    "    header = header_dict[pulse_sequence]\n",
    "    \n",
    "    #import data into numpy array\n",
    "    data = np.loadtxt(infile, usecols = np.arange(0,len(header)))\n",
    "    \n",
    "    # Check if final row is just zeros\n",
    "    if data[-1,0] == 0.:\n",
    "        data = data[:-1,:]\n",
    "    \n",
    "    # Create a pandas dataframe\n",
    "    df_inv = pd.DataFrame(data, columns = header)\n",
    "    \n",
    "    # Add a null column of T2* (FID and T1) or T2 (CPMG)\n",
    "    #if 'T2*' not in header:\n",
    "    #    df_inv['T2*'] = np.nan*np.ones(df_inv.shape[0])\n",
    "    #elif 'T2' not in header:\n",
    "    #    df_inv['T2'] = np.nan*np.ones(df_inv.shape[0])\n",
    "    \n",
    "    \n",
    "    return df_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the index of acquisition entry given site, and  acquisition parameters\n",
    "\n",
    "def get_foreign_keys(df_acquisition, Field_ID, pulse_sequence, pulse_length):\n",
    "    # Define criterion    \n",
    "    criterion1 = df_acquisition['Field_ID'].map(lambda x: x== Field_ID)\n",
    "    criterion2 = df_acquisition['pulse_sequence'].map(lambda x: x== pulse_sequence)\n",
    "    criterion3 = df_acquisition['pulse_length'].map(lambda x: x == pulse_length)\n",
    "\n",
    "    acquisition = df_acquisition[criterion1][criterion2][criterion3]\n",
    "    \n",
    "    return acquisition.site_id, acquisition.index\n",
    "\n",
    "# Function for updating the acquisition metadata dataframe\n",
    "\n",
    "def update_inversion_metadata(inv_id, acqu_ind, doi, \n",
    "                              reg_parameter, cond_profile):\n",
    "    # First we convert the cond profile to a boolean\n",
    "    \n",
    "    if cond_profile == 0:\n",
    "        conductive_earth = False\n",
    "    else:\n",
    "        conductive_earth = True\n",
    "    \n",
    "    d = {'inversion_id': inv_id,\n",
    "         'acquisition_id': acqu_ind, \n",
    "         'DOI': doi,\n",
    "         'conductive_earth': conductive_earth,\n",
    "         'regularisation_parameter': reg_parameter}\n",
    "\n",
    "    # Now add an entry to the df_invmet dataframe\n",
    "\n",
    "    df_temp = df_invmet.append(pd.DataFrame.from_dict([d]),\n",
    "                                ignore_index = True)\n",
    "    return df_temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table for inversion metadata\n",
    "\n",
    "df_invmet = pd.DataFrame(columns =  ['inversion_id', 'acquisition_id', 'DOI',\n",
    "                                     'conductive_earth', 'regularisation_parameter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe into which to add data\n",
    "\n",
    "df_inversion = pd.DataFrame(columns = ['inversion_id', 'Depth_from', 'Depth_to', 'K_rel', \n",
    "                                       'water_content', 'T2*','T2', 'frequency', 'phase', 'T1',\n",
    "                                       'Bound_water_content', 'Mobile_water_content', \n",
    "                                       'Total_water_content', 'acquisition_id', 'site_id'])\n",
    "inv_id = 0\n",
    "\n",
    "# Now we import the inversion data\n",
    "\n",
    "for index, row in df_acquisition.iterrows():\n",
    "    \n",
    "    infile = row['inversion_file']\n",
    "    \n",
    "    try:\n",
    "        df_inv = parse_inversion_file(infile, row.pulse_sequence)\n",
    "    \n",
    "        site_ind, acqu_ind = row['site_id'], index\n",
    "    \n",
    "        reg_parameter = row['reg_factor']\n",
    "        cond_profile = row['use_cond_profile']\n",
    "    \n",
    "    \n",
    "        ##TODO fix this\n",
    "        # Extract the depth of investigation from the matlab header\n",
    "        fig = loadmat('\\\\'.join(infile.split('\\\\')[:-1]) + '\\\\resmatrix.fig')\n",
    "    \n",
    "        # Rather ugly hack to extract the depth of investigation from a \n",
    "        # .fig file\n",
    "\n",
    "        doi = fig['hgS_070000'][0][0][3][1][0][3][1][0][2][0][0][7][0][0]\n",
    "    \n",
    "        # Update the inversion metadata dataframe\n",
    "                \n",
    "        df_invmet = update_inversion_metadata(inv_id, acqu_ind,\n",
    "                                          doi, reg_parameter, cond_profile)\n",
    "    \n",
    "\n",
    "        df_inv['acquisition_id'] = int(acqu_ind)\n",
    "        df_inv['site_id'] = int(site_ind)\n",
    "        df_inv['inversion_id'] = inv_id\n",
    "        \n",
    "        df_inversion = df_inversion.append(df_inv)\n",
    "        inv_id += 1\n",
    "        \n",
    "    except OSError:\n",
    "        \n",
    "        print(row.Field_ID)\n",
    "\n",
    "# Reset the inversion index so t can be used as a primary key\n",
    "df_inversion.reset_index(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invmet.set_index('inversion_id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to get the dataframes into the database classes\n",
    "\n",
    "all_sites = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    site = Sites(site_id = index,\n",
    "                 field_id = row[\"Field_ID\"], \n",
    "                site_code = row[\"Site_Code\"], \n",
    "                mid_x = row[\"mid_X\"], mid_y = row[\"mid_Y\"],\n",
    "                declination = row[\"declination_angle\"],\n",
    "                inclination = row[\"inclination_angle\"],\n",
    "                date = pd.to_datetime(row['date'], format = '%d/%m/%Y').date(),\n",
    "                loop_width = row[\"loop_width\"],\n",
    "                coil_type = row[\"coil_type\"],\n",
    "                 geometry = row['geometry'])\n",
    "    all_sites.append(site)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to get the dataframes into the database classes\n",
    "\n",
    "all_acquisitions = []\n",
    "\n",
    "for index, row in df_acquisition.iterrows():\n",
    "    acquisition = Acquisitions(acquisition_id = index,\n",
    "                               pulse_sequence = row['pulse_sequence'],\n",
    "                               pulse_length = row['pulse_length'],\n",
    "                               site_id = row['site_id'])\n",
    "                               \n",
    "    all_acquisitions.append(acquisition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inversion_metadata = []\n",
    "\n",
    "for index, row in df_invmet.iterrows():\n",
    "    inv_met = Inverse_model_metadata(doi = row['DOI'],\n",
    "                                     reg_factor = row['regularisation_parameter'],\n",
    "                                     cond_profile = True,\n",
    "                                     inversion_software = 'GMRInversion1D_CLI_v2.7.3',\n",
    "                                     acquisition_id = row['acquisition_id'],\n",
    "                                     inversion_id = index)\n",
    "    all_inversion_metadata.append(inv_met)\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_inversions = []\n",
    "\n",
    "for index, row in df_inversion.iterrows():\n",
    "    inversion = Inverse_models(table_id = index,\n",
    "                               depth_from = row['Depth_from'],\n",
    "                           depth_to = row['Depth_to'],\n",
    "                           mobile_water_content = row['Mobile_water_content'],\n",
    "                           bound_water_content = row[\"Bound_water_content\"],\n",
    "                           total_water_content =row[\"Total_water_content\"],\n",
    "                           T1 = row[\"T1\"],\n",
    "                           T2 = row[\"T2\"],\n",
    "                           T2_star = row[\"T2*\"],\n",
    "                           frequency = row[\"frequency\"],\n",
    "                           phase = row[\"phase\"],\n",
    "                           site_id = row['site_id'],\n",
    "                           acquisition_id = row['acquisition_id'],\n",
    "                              inversion_id = row['inversion_id'])\n",
    "                               \n",
    "    all_inversions.append(inversion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_all(all_sites)\n",
    "session.add_all(all_acquisitions)\n",
    "session.add_all(all_inversions)\n",
    "session.add_all(all_inversion_metadata)\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a Spatialite geometry column called 'geom' to the table, using ESPG 28352,\n",
    "# data type POLYGON and 2 dimensions (x, y)\n",
    "engine.execute(\"SELECT AddGeometryColumn('sites', 'geom', 28352, 'POLYGON', 2);\")\n",
    "\n",
    "# update the yet empty geom column by parsing the well-known-binary objects from the geometry column into \n",
    "# Spatialite geometry objects\n",
    "engine.execute(\"UPDATE sites SET geom=GeomFromText(geometry, 28352);\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
