{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for producing inversion ready AEM data files (for use with Ross C brodie's inversion codes) from a netCDF4 data and recalculated noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlite3 import dbapi2 as sqlite\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, event\n",
    "from geophys_utils import NetCDFPointUtils\n",
    "from hydrogeol_utils import spatial_functions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the AEM data file\n",
    "nc_inpath = r\"C:\\Users\\PCUser\\Desktop\\AEM\\EM\\AUS_10021_DalyR_EM.nc\"\n",
    "\n",
    "d = netCDF4.Dataset(nc_inpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "910283.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(d['easting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to load a number of coordiantes that we will be inverting the nearest neighbour\n",
    "\n",
    "# Extract borehole data from the database\n",
    "\n",
    "DB_PATH = r\"C:\\Users\\PCUser\\Desktop\\EK_data\\Boreholes\\East_Kimberley_borehole_data.sqlite\"\n",
    "\n",
    "engine = db.create_engine('sqlite:///' + DB_PATH, module=sqlite)\n",
    "\n",
    "connection = engine.connect()\n",
    "\n",
    "# Open the borehole data as a pandas dataframe\n",
    "\n",
    "df_header =pd.read_sql('select Easting, Northing from borehole', connection)\n",
    "\n",
    "# Now do the same for SNMR sites\n",
    "\n",
    "DB_PATH = r\"C:\\Users\\PCUser\\Desktop\\EK_data\\SNMR\\East_Kimberley_SNMR.sqlite\"\n",
    "\n",
    "engine = db.create_engine('sqlite:///' + DB_PATH, module=sqlite)\n",
    "\n",
    "connection = engine.connect()\n",
    "\n",
    "# Open the borehole data as a pandas dataframe\n",
    "\n",
    "df_SNMR =pd.read_sql('select mid_X, mid_Y from sites', connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates in an array\n",
    "\n",
    "coords = np.zeros(shape = (len(df_header) + len(df_SNMR), 2),\n",
    "                 dtype = np.float64)\n",
    "\n",
    "coords[:len(df_header),:] = df_header.values\n",
    "\n",
    "coords[len(df_header):,:] = df_SNMR.values\n",
    "\n",
    "df_coords = pd.DataFrame(coords, columns = ['Easting', 'Northing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_point_utils = NetCDFPointUtils(d)\n",
    "\n",
    "# Get the AEM utm coordinates\n",
    "\n",
    "aem_coords = np.column_stack((d['easting'][:], d['northing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the AEM conductivity using nearest neighbour\n",
    "distances, indices = spatial_functions.nearest_neighbours(coords,\n",
    "                                                          aem_coords,\n",
    "                                                          points_required = 1,# return 10 closest points\n",
    "                                                          max_distance = 250.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_inds = np.unique(indices[np.isfinite(distances)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = d['line'][:].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100001, 100101, 100201, 100301, 100401, 100501, 100601, 100701,\n",
       "       100801, 100901, 101001, 101101, 101201, 101301, 101401, 101501,\n",
       "       101502, 101601, 101701, 101801, 101901, 102001, 102101, 102201,\n",
       "       102301, 102401, 102501, 102601, 102701, 102801, 102901, 103001,\n",
       "       103101, 103201, 103301, 103401, 103501, 103502, 103601, 103701,\n",
       "       103801, 103901, 104001, 104101, 104102, 104201, 104301, 104302,\n",
       "       104401, 104501, 104601, 104701, 104801, 104901, 105001, 105101,\n",
       "       105201, 105301, 105401, 105501, 105601, 105602, 105603, 105701,\n",
       "       105801, 105901, 105902, 106001, 106101, 106201, 106301, 106302,\n",
       "       106401, 106501, 106601, 106701, 106801, 106901, 107001, 107101,\n",
       "       107201, 107301, 107401, 107501, 107601, 107701, 107801, 107901,\n",
       "       108001, 108101, 108201, 108301, 108401, 108501, 108601, 108701,\n",
       "       108801, 108901, 109001, 109101, 109201, 109202, 109203, 109204,\n",
       "       109301, 109302, 109303, 109401, 109402, 109501, 109502, 109503,\n",
       "       109504, 109601, 109701, 109702, 109703, 109801, 109802, 109803,\n",
       "       109901, 110001, 110002, 110003, 110101, 110102, 110103, 110201,\n",
       "       110202, 110203, 110301, 110401, 110501, 110601, 110701, 110801,\n",
       "       110901, 111001, 912001, 912002, 912003, 912004, 912005, 913001,\n",
       "       913002, 913003, 913004, 913005, 913006, 913007, 913008, 913009,\n",
       "       913010, 913011, 913012, 913013, 913014, 913015, 913016, 913017,\n",
       "       913018, 913019, 913020, 913021, 913022, 913023, 913024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-37934517e995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get a subset of the Keep data from line 300,000-400,000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mKeep_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m300000.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m400000.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mKeep_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcond_point_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_lookup_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeep_lines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "# get a subset of the Keep data from line 300,000-400,000\n",
    "\n",
    "Keep_lines = [x for x in lines if np.logical_and(x>100000., x<400000.)]\n",
    "\n",
    "Keep_mask = cond_point_utils.get_lookup_mask(Keep_lines)\n",
    "\n",
    "Keep_inds = np.where(Keep_mask)[0]\n",
    "\n",
    "# Get 10,000 random points\n",
    "\n",
    "#np.random.shuffle(Keep_inds)\n",
    "\n",
    "masked_inds = Keep_inds#[:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167007"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['northing'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for these points we want to extract data into a .dat file with formatting defined by a dfn\n",
    "\n",
    "\n",
    "\n",
    "cols = [\"ga_project\", \"utc_date\", \"flight\", \"line\", \"fiducial\", \"easting\", \"northing\",\n",
    "        \"tx_height_measured\", \"elevation\", \"gps_height\", \"roll\", \"pitch\", \"yaw\",\n",
    "        \"TxRx_dx\", \"TxRx_dy\", \"TxRx_dz\", \"low_moment_Z-component_EM_data\",\"high_moment_Z-component_EM_data\",\n",
    "             \"lm_z_noise\", \"hm_z_noise\"]\n",
    "inv_read = {}\n",
    "\n",
    "for item in cols:\n",
    "    # Scalar variables\n",
    "    if len(d[item].shape) == 0:\n",
    "        inv_read[item] = d[item][:].data\n",
    "    # Vectors\n",
    "    elif len(d[item].shape) == 1:\n",
    "        if item == 'line':\n",
    "            line_inds = d['line_index'][:]#[masked_inds]\n",
    "            inv_read[item] = d[item][line_inds].data\n",
    "        elif item == 'flight':\n",
    "            flight_inds = d['flight_index'][:]#[masked_inds]\n",
    "            inv_read[item] = d[item][flight_inds].data            \n",
    "        else:\n",
    "            inv_read[item] = d[item][:].data#[masked_inds].data\n",
    "    # Arrays\n",
    "    elif len(d[item].shape) == 2:\n",
    "        inv_read[item] = d[item][:].data#[masked_inds].data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inv_read['lm_z_noise'] = 0.5*inv_read['lm_z_noise']\n",
    "#inv_read['hm_z_noise'] = 0.5*inv_read['hm_z_noise']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index = range(d['northing'].shape[0]))#masked_inds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in inv_read:\n",
    "    if len(inv_read[item].shape) < 2:\n",
    "        df[item] = inv_read[item]\n",
    "    else:\n",
    "        a = inv_read[item]\n",
    "        for i in range(a.shape[1]):\n",
    "            df[item + '_' + str(i+1)] = a[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any entries with high altitude lines\n",
    "\n",
    "df = df[df['line'] < 913000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to resave these columns as strings with a set format\n",
    "\n",
    "# Now we replace the columns with formatted strings\n",
    "\n",
    "df.at[:,'ga_project'] = ['{:5d}'.format(x) for x in df['ga_project'].values.astype(int)]\n",
    "df.at[:,'utc_date'] = ['{:9.0F}'.format(x) for x in df['utc_date'].values]\n",
    "df.at[:,'flight'] = ['{:12.2F}'.format(x) for x in df['flight'].values]\n",
    "df.at[:,'line'] = ['{:8.0F}'.format(x) for x in df['line'].values]\n",
    "df.at[:,'fiducial'] = ['{:12.2F}'.format(x) for x in df['fiducial'].values]\n",
    "df.at[:,'easting'] = ['{:10.2F}'.format(x) for x in df['easting'].values]\n",
    "df.at[:,'northing'] = ['{:11.2F}'.format(x) for x in df['northing'].values]\n",
    "df.at[:,'tx_height_measured'] = ['{:8.1F}'.format(x) for x in df['tx_height_measured'].values]\n",
    "df.at[:,'elevation'] = ['{:9.2F}'.format(x) for x in df['elevation'].values]\n",
    "df.at[:,'gps_height'] = ['{:9.2F}'.format(x) for x in df['gps_height'].values]\n",
    "df.at[:,'roll'] = ['{:7.2F}'.format(x) for x in df['roll'].values]\n",
    "df.at[:,'pitch'] = ['{:7.2F}'.format(x) for x in df['pitch'].values]\n",
    "df.at[:,'yaw'] = ['{:7.2F}'.format(x) for x in df['yaw'].values]\n",
    "df.at[:,'TxRx_dx'] = ['{:7.2F}'.format(x) for x in df['TxRx_dx'].values]\n",
    "df.at[:,'TxRx_dy'] = ['{:7.2F}'.format(x) for x in df['TxRx_dy'].values]\n",
    "df.at[:,'TxRx_dz'] = ['{:7.2F}'.format(x) for x in df['TxRx_dz'].values]\n",
    "\n",
    "\n",
    "# Iterate through the the data\n",
    "\n",
    "for item in df.columns[16:]:\n",
    "    df.at[:,item] = ['{:15.6E}'.format(x) for x in df[item].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we output the data\n",
    "outfile = r\"C:\\Users\\PCUser\\Desktop\\NSC_data\\data\\AEM\\DR\\2017_DalyRiver_SkyTEM\\inversion_ready\\DR_temp.dat\"\n",
    "\n",
    "# Note use a pipe so we can easily delete later\n",
    "df.to_csv(outfile, sep = '|', index = False, header = False)\n",
    "\n",
    "# Now opent the file and delete the pipe\n",
    "\n",
    "with open(outfile, 'r') as inf:\n",
    "    s = inf.read()\n",
    "\n",
    "new_s = s.replace('|','')\n",
    "\n",
    "\n",
    "# Reomve the final\n",
    "if new_s[-1:] == '\\n':\n",
    "    new_s = new_s[:-1]\n",
    "\n",
    "new_outfile = r\"C:\\Users\\PCUser\\Desktop\\NSC_data\\data\\AEM\\DR\\2017_DalyRiver_SkyTEM\\inversion_ready\\DR_inversion_ready.dat\"\n",
    "\n",
    "with open(new_outfile, 'w') as f:\n",
    "    f.write(new_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we output the data\n",
    "outfile = r\"C:\\Users\\PCUser\\Desktop\\EK_data\\AEM\\inversion_ready_data\\OrdKeep_inversion_ready_subset_temp.dat\"\n",
    "\n",
    "# Note use a pipe so we can easily delete later\n",
    "df.iloc[0].to_csv(outfile, sep = '|', index = False, header = False)\n",
    "\n",
    "# Now opent the file and delete the pipe\n",
    "\n",
    "with open(outfile, 'r') as inf:\n",
    "    s = inf.read()\n",
    "\n",
    "new_s = s.replace('|','')\n",
    "\n",
    "\n",
    "# Reomve the final\n",
    "if new_s[-1:] == '\\n':\n",
    "    new_s = new_s[:-1]\n",
    "\n",
    "new_outfile = r\"C:\\Users\\PCUser\\Desktop\\EK_data\\AEM\\inversion_ready_data\\OrdKeep_inversion_ready_onefid.dat\"\n",
    "\n",
    "with open(new_outfile, 'w') as f:\n",
    "    f.write(new_s)\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
